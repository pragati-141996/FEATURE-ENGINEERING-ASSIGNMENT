{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT QUESTIONS"
      ],
      "metadata": {
        "id": "0IOe1djVM6gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?"
      ],
      "metadata": {
        "id": "avPhomRJM_wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Feature engineering is the process of:selecting,transforming, or creating new features (columns) from raw data\n",
        "to improve the performance of a machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "jtYm606qNn3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?What does negative correlation mean?"
      ],
      "metadata": {
        "id": "5tMuhHApN23z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  In feature engineering, correlation helps us understand how strongly one feature (input) is related to another feature or to the target variable (output).It’s used to analyze relationships between variables.Especially useful in feature selection: choosing which features to keep or remove.\n",
        "-  Negative correlation means:As one feature increases, another feature or the target decreases."
      ],
      "metadata": {
        "id": "kqGbCrWuN9F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "egpbqDogOyAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Machine Learning is a field of artificial intelligence (AI) that focuses on building systems that learn from data, identify patterns, and make decisions with minimal human intervention.\n",
        "-  MAIN COMPONENTS:\n",
        "-  Data:The foundation of machine learning.Dat can be structured (tables) or unstructured (text, images, audio).It needs to be cleaned, preprocessed, and sometimes labeled.\n",
        "-  Features:Inputs (also called variables or attributes) used by the model to make predictions.Feature engineering improves model performance by selecting, transforming, or creating features.\n",
        "-  Model:A mathematical representation or algorithm that learns patterns from data.Common types: Linear Regression, Decision Tree, Neural Network, etc.\n",
        "-  Training:The process of feeding data to the model so it can learn the relationship between features and target.During training, the model adjusts internal parameters (like weights).\n",
        "-   Evaluation:After training, the model is tested using a separate dataset (test set) to measure performance.Common metrics: Accuracy, Precision, Recall, RMSE, etc.\n"
      ],
      "metadata": {
        "id": "5l620FTYO3Zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "kXBpt2K6RVM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The loss value in machine learning is a key indicator of how well a model is performing. It measures the difference between the predicted outputs and the actual target values using a loss function. A lower loss value means the model's predictions are close to the true values, indicating better performance, while a higher loss value shows that the model is making more errors. During training, the model adjusts its internal parameters to minimize this loss, which helps it learn from the data. Monitoring the loss on both the training and validation datasets helps in determining whether the model is underfitting, overfitting, or performing well. For example, if the training loss is low but the validation loss is high, the model may be overfitting — learning the training data too well but failing to generalize to new data. Therefore, the loss value plays a crucial role in evaluating, comparing, and improving machine learning models."
      ],
      "metadata": {
        "id": "mFHHLpBKRaqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "qjFv2Nh_S56f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A continuous variable is a numeric variable that can take any value within a range. The values are measurable and often represent quantities.These variables can have decimals and an infinite number of possible values within a range.e.g. height 170.5 cm.\n",
        "\n",
        "-  A categorical variable represents categories, labels, or groups. These are typically non-numeric, or if numeric, the numbers represent categories, not quantities.Types of Categorical Variables:\n",
        "-  Nominal (no order):Examples: Gender (Male, Female), Color (Red, Blue)\n",
        "-  Ordinal (with order):Examples: Education Level (High School < Bachelor < Master), Customer Rating (Low, Medium, High)."
      ],
      "metadata": {
        "id": "gyE9r17rS9_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "3B2G55xCUaK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Machine learning models require numerical input, so categorical variables must be converted into numbers before training. This process is called encoding. Choosing the right technique depends on the type of categorical data (nominal or ordinal) and the algorithm being used.\n",
        "-  COMMON TECHNIQUES:\n",
        "-  Label Encoding : assigns a unique integer to each category(e.g., \"Male\" → 0, \"Female\" → 1). While simple, it’s best used only when the categories have a meaningful order (ordinal data), as it can incorrectly suggest a ranking in nominal variables.\n",
        "-  One-Hot Encoding : is another widely used technique, especially for nominal data. It creates separate binary columns for each category, ensuring that no artificial order is introduced, though it can increase the number of features significantly in datasets with many categories.Example:\"Red\", \"Blue\", \"Green\" → becomes 3 columns: is_Red, is_Blue, is_Green.\n",
        "-  Ordinal Encoding : Assigns numeric values based on order in ordinal categories.We must define the order manually.\n",
        "-  Frequency Encoding : Replaces each category with its frequency in the dataset.Example: If \"A\" appears 100 times, \"B\" appears 50 → A = 100, B = 50.\n",
        "-  Target Encoding (Mean Encoding) : Replaces each category with the mean of the target variable for that category.\n",
        "-  Binary Encoding / Hash Encoding : Useful for high-cardinality features (many unique categories).It combines the benefits of label and one-hot encoding but with fewer columns.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ElVu2tuiUeXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "29aE1cgvYNWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Training and testing a dataset are two fundamental steps in building and evaluating a machine learning model.\n",
        "-  Training a dataset means using a portion of your data to teach the model to learn patterns, relationships, or rules. During training, the model processes the input features and adjusts its internal parameters (like weights) to minimize errors in predicting the target outcome. This step is where the model “learns” from examples.\n",
        "-  Testing a dataset, on the other hand, involves evaluating the trained model on a separate portion of data that it hasn’t seen before. The goal is to check how well the model generalizes—meaning how accurately it can make predictions on new, unseen data. Testing helps detect problems like overfitting, where a model performs well on training data but poorly on new data.\n",
        "-  Typically, the original dataset is split into at least two parts: the training set (used to train the model) and the test set (used to evaluate its performance). This division ensures that the model’s accuracy is measured fairly and reflects its real-world effectiveness.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0BTnKooAYTIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "Hmp80zdsY4DB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  sklearn.preprocessing is a module in the scikit-learn library (a popular Python library for machine learning) that provides a variety of tools and functions to preprocess and transform data before feeding it into machine learning models.Preprocessing is an essential step because raw data often contains values in different scales, formats, or types that models can’t handle directly or that might negatively affect model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "GMWyx1k7Y8vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "Maa9rO-Xk2wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A test set is a portion of the dataset that is kept separate from the training data and is used only to evaluate the final performance of your trained machine learning model."
      ],
      "metadata": {
        "id": "xyaonRSbk6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "bIcxoR36lKdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  To split data for model fitting in Python, we commonly use the train_test_split() function from the sklearn.model_selection module. This function allows us to divide the dataset into two parts: one for training the model and another for testing its performance. For example, you can assign 80% of the data to the training set and the remaining 20% to the test set by specifying test_size=0.2.This split is crucial because it helps evaluate how well the model generalizes to unseen data.\n",
        "-  Approaching a machine learning problem involves a structured step-by-step process. Here's a standard workflow:\n",
        "-  Understand the Problem : Identify whether it's a classification, regression, or clustering task.Understand business goals and target variable.\n",
        "-  Collect and Explore Data (EDA) : Load the dataset.Use tools like pandas, matplotlib, and seaborn to explore:Data types,missing values,class imbalance,outliers,feature relationships (correlation, distributions)\n",
        "-  Preprocess the Data : handle missing values,encode categorical variables,scale/normalize features,feature selection or dimensionality reduction (e.g., PCA).\n",
        "-  Split the Data : Use train_test_split() to divide the data into training and testing sets.\n",
        "-  Choose a Model : Try suitable algorithms (e.g., Logistic Regression, Decision Tree, Random Forest, etc.)Train the model using fit().\n",
        "-  Evaluate the Model : Use the test set to evaluate performance.Apply metrics:Classification: Accuracy, Precision, Recall, F1-score and Regression: RMSE, MAE, R².\n",
        "-  Tune Hyperparameters : Use techniques like GridSearchCV or RandomizedSearchCV to find the best parameters.\n",
        "-   Finalize and Deploy : Train the model on the full dataset (if appropriate).Save the model (joblib, pickle).Deploy it as an API or integrate it into applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8xjxzBg2lSTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "3tL5BgzlnvwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  We perform Exploratory Data Analysis (EDA) before fitting a model to the data because it helps us understand the structure, quality, and patterns within the dataset, which is crucial for building an effective machine learning model.EDA helps us identify missing values, outliers, and errors in the data that could mislead the model if left unaddressed. It reveals the distribution of variables, allowing us to decide if transformations (e.g., log-scaling) are necessary. EDA also shows relationships between variables—like correlations or trends—that can guide feature selection and engineering. It helps us distinguish between categorical and numerical variables, so we can apply appropriate preprocessing techniques like encoding or normalization. Moreover, EDA gives us insight into the balance of the dataset, such as whether the target variable is imbalanced, which affects model choice and evaluation strategies."
      ],
      "metadata": {
        "id": "VKpZQAyPn0oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?"
      ],
      "metadata": {
        "id": "bhQKEyL8qFDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  SAME QUESTION REPEATED TWICE"
      ],
      "metadata": {
        "id": "o6w056TWqLVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "RqQR-RgeqP9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   SAME QUESTION REPEATED TWICE"
      ],
      "metadata": {
        "id": "E_3GHFrHqTTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "SZkInLp6qXfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   In Python, we can find the correlation between variables using the Pandas library. After loading the dataset into a Pandas DataFrame, we use the .corr() method to compute the correlation matrix, which displays the pairwise correlation coefficients between all numerical columns. By default, this method calculates the Pearson correlation, which measures the linear relationship between variables, but we can also choose other methods such as 'kendall' or 'spearman' depending on the type of data and the relationship we want to explore. To gain clearer insights, we often visualize the correlation matrix using a heatmap with the Seaborn library. This graphical representation helps us quickly identify strong positive or negative correlations. Performing correlation analysis allows us to detect redundant features, understand variable relationships, and avoid multicollinearity—ultimately guiding us in selecting the right features before fitting a machine learning model."
      ],
      "metadata": {
        "id": "zOZaTPUdq2mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "aTHMrkSpsB1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   Causation refers to a relationship where one variable directly affects another.The difference between correlation and causation is that correlation shows a statistical relationship (two variables move together), but it does not mean that one variable causes the other to change. Causation, on the other hand, means one variable produces a change in another.\n",
        "-   Imagine we observe that ice cream sales and drowning incidents both increase during the summer. This is a correlation — as one increases, so does the other. However, buying ice cream does not cause drowning. The real causal factor here is hot weather, which increases both swimming activity and ice cream consumption. So, while the two variables are correlated, they do not have a causal relationship."
      ],
      "metadata": {
        "id": "h7ccA_1gsISL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "Wm8ot5Drsv38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   An optimizer is a key component in machine learning, especially in deep learning, that is used to minimize the loss function by adjusting the model’s parameters (like weights and biases). The goal of an optimizer is to find the best set of parameters that results in the most accurate predictions.\n",
        "-   TYPES OF OPTIMIZERS :\n",
        "-    Gradient Descent (GD) : This is the most basic optimizer. It calculates the gradient of the loss function with respect to each parameter for the entire dataset, and updates all parameters simultaneously.Eg: If loss = (w - 3)^2 and w = 0,\n",
        "Gradient = -6 ⇒ w = w - 0.1 * (-6) ⇒ w = 0.6\n",
        "\n",
        "-   Stochastic Gradient Descent (SGD) : Instead of using the entire dataset, SGD uses one data point at a time to update parameters. It’s faster but introduces noise in the updates.\n",
        "-    Mini-Batch Gradient Descent : It combines the benefits of GD and SGD by using a small batch of data for each update. It provides a balance between speed and accuracy.Example: Commonly used in deep learning libraries like TensorFlow or PyTorch using batch_size.\n",
        "-   Momentum Optimizer : This builds on SGD by adding a “momentum” term that helps the optimizer move faster in the relevant direction and avoid local minima.\n",
        "-   RMSProp (Root Mean Square Propagation) : This optimizer adapts the learning rate for each parameter, using a moving average of squared gradients to dampen oscillations.\n",
        "-   Adam (Adaptive Moment Estimation) : Combines the ideas of Momentum and RMSProp. It keeps track of both first moment (mean) and second moment (variance) of the gradients. It’s the most widely used optimizer in deep learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "2sYDoyO0s1PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Examples\n",
        "#Stochastic Gradient Descent (SGD)\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "#Momentum Optimizer\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "#RMSProp (Root Mean Square Propagation)\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "# Adam (Adaptive Moment Estimation)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "1qozdrIlyb6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "k_wM-FiFzaHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   sklearn.linear_model is a submodule of the scikit-learn machine learning library in Python. It provides a collection of linear models that are used to solve both regression and classification problems. These models make predictions by assuming a linear relationship between the input features (independent variables) and the target output (dependent variable).\n"
      ],
      "metadata": {
        "id": "Hb8WD1wXzeng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "VDqEv7pW0wrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   The .fit() method is one of the most important functions in the Scikit-learn library. It is used to train a machine learning model on a labeled dataset, meaning it adjusts the model's internal parameters based on the input data (X) and the corresponding correct output (y) so that it can learn patterns and make accurate predictions.\n",
        "-   The model.fit() method in scikit-learn takes primarily two required arguments: X and y. The X argument represents the input features and must be in an array-like structure (such as a NumPy array, pandas DataFrame, or list of lists), where each row corresponds to a single sample and each column corresponds to a feature. The shape of X is typically (n_samples, n_features). The y argument represents the target values or labels corresponding to each sample in X. It can be a one-dimensional array (for single-output regression or binary classification) or a multi-dimensional array for multi-output tasks. The shape of y is usually (n_samples,) or (n_samples, n_outputs)."
      ],
      "metadata": {
        "id": "v3F-PRT203Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "afolNauS1UBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   The model.predict() method in scikit-learn is used to make predictions on new, unseen data after a model has been trained using model.fit().\n",
        "Once the model has learned patterns from the training data, predict() takes in a set of input features (X_new) and returns the model’s estimated output — either a continuous value for regression problems or a class label for classification problems.\n",
        "The model.predict() method in scikit-learn requires a single, essential argument: X. This argument represents the input data for which we want the model to make predictions. It should be provided in an array-like format such as a NumPy array, a pandas DataFrame, or a list of lists. The structure of X must match the format used during training, meaning it should have the same number of features (columns) as the data passed to model.fit(). Each row in X corresponds to a single sample or observation, and the shape of the input is typically (n_samples, n_features).There are no other mandatory arguments for predict() in most scikit-learn models.\n",
        "\n"
      ],
      "metadata": {
        "id": "BzP_q4i87x9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "AwBc_XNKDBEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   SAME QUESTION REPEATED TWICE"
      ],
      "metadata": {
        "id": "AGOU0_2fDJNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "NcQy5u0lDUOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   Feature Scaling is a technique in Machine Learning used to normalize or standardize the range of independent variables or features of data. It ensures that all features contribute equally to model training and are on a comparable scale.\n",
        "-   Usefulness in Machine Learning:\n",
        "-   Improves Model Performance : Algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Gradient Descent-based models (e.g., Logistic Regression, Neural Networks) perform better when features are scaled.If not scaled, features with large values can dominate those with smaller ranges.\n",
        "-   Faster Convergence : In optimization algorithms like Gradient Descent, feature scaling ensures faster and more stable convergence, as the steps taken in each dimension become more uniform.\n",
        "-   Avoids Numerical Instability : Models involving matrix operations (like in linear regression or PCA) are numerically more stable when input data is scaled.\n",
        "-   Ensures Equal Feature Importance : Prevents one feature from dominating others just because of its scale, leading to more balanced models."
      ],
      "metadata": {
        "id": "QPB6oAZnDcl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n"
      ],
      "metadata": {
        "id": "7tULn20cqIsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   In Python, we perform feature scaling using the sklearn.preprocessing module from the scikit-learn library. First, we choose a scaling technique based on our needs. If we want to scale our features to a fixed range, usually between 0 and 1, we use MinMaxScaler. If we prefer to standardize our data so that it has a mean of 0 and a standard deviation of 1, we use StandardScaler. For data that contains outliers, we use RobustScaler, which scales the data based on the median and interquartile range.We begin by creating a scaler object, then use the fit_transform() method on our training data. This allows the scaler to learn the parameters (like minimum and maximum or mean and standard deviation) and apply the scaling. When we work with test data, we only apply the transform() method using the same scaler to prevent data leakage. After scaling, the data becomes more suitable for machine learning models that are sensitive to feature magnitudes, such as K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and logistic regression. We often convert the scaled NumPy array back into a DataFrame to make it easier to understand and work with."
      ],
      "metadata": {
        "id": "7kG-R1K1qQFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "PrWzA2Q5smYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   SAME QUESTION REPEATED TWICE"
      ],
      "metadata": {
        "id": "2vQW9EvDsvQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "7eIqjOWls8uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   SAME QUESTION REPEATED TWICE"
      ],
      "metadata": {
        "id": "vixdurg0tAGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding."
      ],
      "metadata": {
        "id": "QFjcTX2QtM8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   Data encoding is the process of converting categorical variables (which are often non-numeric) into a numerical format that can be understood and processed by machine learning models. Most algorithms can only work with numbers, so encoding is a critical step in the data preprocessing pipeline when dealing with categorical data."
      ],
      "metadata": {
        "id": "R9mIMamAtUz5"
      }
    }
  ]
}